{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cfb43af",
   "metadata": {},
   "source": [
    "# Assignment 4: Pipelines and Text Data (60 total marks)\n",
    "### Due: March 21 at 11:59pm\n",
    "\n",
    "### Name: Maryam Alsayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "165f8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5ff9ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #ignoring some deprication warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf05c7e",
   "metadata": {},
   "source": [
    "## Part 1: Pipelines (26 marks)\n",
    "\n",
    "The purpose of this part of the assignment is to practice following the grid-search workflow: \n",
    "- Split data into training and test set\n",
    "- Use the training portion to find the best model using grid search and cross-validation\n",
    "- Retrain the best model\n",
    "- Evaluate the retrained model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31108625",
   "metadata": {},
   "source": [
    "### 1.1: Load data (4 marks)\n",
    "For this task, we will be using stock data from the Dow Jones Index. This dataset uses information about different stocks to try to predict what the percent change in price will be from week to week.\n",
    "\n",
    "More information on the dataset can be found here: https://archive.ics.uci.edu/dataset/312/dow+jones+index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5b1e2cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quarter</th>\n",
       "      <th>stock</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>percent_change_price</th>\n",
       "      <th>percent_change_volume_over_last_wk</th>\n",
       "      <th>previous_weeks_volume</th>\n",
       "      <th>next_weeks_open</th>\n",
       "      <th>next_weeks_close</th>\n",
       "      <th>percent_change_next_weeks_price</th>\n",
       "      <th>days_to_next_dividend</th>\n",
       "      <th>percent_return_next_dividend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/7/2011</td>\n",
       "      <td>$15.82</td>\n",
       "      <td>$16.72</td>\n",
       "      <td>$15.78</td>\n",
       "      <td>$16.42</td>\n",
       "      <td>239655616</td>\n",
       "      <td>3.79267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$16.71</td>\n",
       "      <td>$15.97</td>\n",
       "      <td>-4.428490</td>\n",
       "      <td>26</td>\n",
       "      <td>0.182704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/14/2011</td>\n",
       "      <td>$16.71</td>\n",
       "      <td>$16.71</td>\n",
       "      <td>$15.64</td>\n",
       "      <td>$15.97</td>\n",
       "      <td>242963398</td>\n",
       "      <td>-4.42849</td>\n",
       "      <td>1.380223</td>\n",
       "      <td>239655616.0</td>\n",
       "      <td>$16.19</td>\n",
       "      <td>$15.79</td>\n",
       "      <td>-2.470660</td>\n",
       "      <td>19</td>\n",
       "      <td>0.187852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/21/2011</td>\n",
       "      <td>$16.19</td>\n",
       "      <td>$16.38</td>\n",
       "      <td>$15.60</td>\n",
       "      <td>$15.79</td>\n",
       "      <td>138428495</td>\n",
       "      <td>-2.47066</td>\n",
       "      <td>-43.024959</td>\n",
       "      <td>242963398.0</td>\n",
       "      <td>$15.87</td>\n",
       "      <td>$16.13</td>\n",
       "      <td>1.638310</td>\n",
       "      <td>12</td>\n",
       "      <td>0.189994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/28/2011</td>\n",
       "      <td>$15.87</td>\n",
       "      <td>$16.63</td>\n",
       "      <td>$15.82</td>\n",
       "      <td>$16.13</td>\n",
       "      <td>151379173</td>\n",
       "      <td>1.63831</td>\n",
       "      <td>9.355500</td>\n",
       "      <td>138428495.0</td>\n",
       "      <td>$16.18</td>\n",
       "      <td>$17.14</td>\n",
       "      <td>5.933250</td>\n",
       "      <td>5</td>\n",
       "      <td>0.185989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>2/4/2011</td>\n",
       "      <td>$16.18</td>\n",
       "      <td>$17.39</td>\n",
       "      <td>$16.18</td>\n",
       "      <td>$17.14</td>\n",
       "      <td>154387761</td>\n",
       "      <td>5.93325</td>\n",
       "      <td>1.987452</td>\n",
       "      <td>151379173.0</td>\n",
       "      <td>$17.33</td>\n",
       "      <td>$17.37</td>\n",
       "      <td>0.230814</td>\n",
       "      <td>97</td>\n",
       "      <td>0.175029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quarter stock       date    open    high     low   close     volume  \\\n",
       "0        1    AA   1/7/2011  $15.82  $16.72  $15.78  $16.42  239655616   \n",
       "1        1    AA  1/14/2011  $16.71  $16.71  $15.64  $15.97  242963398   \n",
       "2        1    AA  1/21/2011  $16.19  $16.38  $15.60  $15.79  138428495   \n",
       "3        1    AA  1/28/2011  $15.87  $16.63  $15.82  $16.13  151379173   \n",
       "4        1    AA   2/4/2011  $16.18  $17.39  $16.18  $17.14  154387761   \n",
       "\n",
       "   percent_change_price  percent_change_volume_over_last_wk  \\\n",
       "0               3.79267                                 NaN   \n",
       "1              -4.42849                            1.380223   \n",
       "2              -2.47066                          -43.024959   \n",
       "3               1.63831                            9.355500   \n",
       "4               5.93325                            1.987452   \n",
       "\n",
       "   previous_weeks_volume next_weeks_open next_weeks_close  \\\n",
       "0                    NaN          $16.71           $15.97   \n",
       "1            239655616.0          $16.19           $15.79   \n",
       "2            242963398.0          $15.87           $16.13   \n",
       "3            138428495.0          $16.18           $17.14   \n",
       "4            151379173.0          $17.33           $17.37   \n",
       "\n",
       "   percent_change_next_weeks_price  days_to_next_dividend  \\\n",
       "0                        -4.428490                     26   \n",
       "1                        -2.470660                     19   \n",
       "2                         1.638310                     12   \n",
       "3                         5.933250                      5   \n",
       "4                         0.230814                     97   \n",
       "\n",
       "   percent_return_next_dividend  \n",
       "0                      0.182704  \n",
       "1                      0.187852  \n",
       "2                      0.189994  \n",
       "3                      0.185989  \n",
       "4                      0.175029  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Load the dataset into a dataframe called stock_data (0.5 marks)\n",
    "stock_data = pd.read_csv('dow_jones_index.data')\n",
    "# TO DO: Inspect the first few columns (0.5 marks)\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8969a634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data types:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750 entries, 0 to 749\n",
      "Data columns (total 16 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   quarter                             750 non-null    int64  \n",
      " 1   stock                               750 non-null    object \n",
      " 2   date                                750 non-null    object \n",
      " 3   open                                750 non-null    object \n",
      " 4   high                                750 non-null    object \n",
      " 5   low                                 750 non-null    object \n",
      " 6   close                               750 non-null    object \n",
      " 7   volume                              750 non-null    int64  \n",
      " 8   percent_change_price                750 non-null    float64\n",
      " 9   percent_change_volume_over_last_wk  720 non-null    float64\n",
      " 10  previous_weeks_volume               720 non-null    float64\n",
      " 11  next_weeks_open                     750 non-null    object \n",
      " 12  next_weeks_close                    750 non-null    object \n",
      " 13  percent_change_next_weeks_price     750 non-null    float64\n",
      " 14  days_to_next_dividend               750 non-null    int64  \n",
      " 15  percent_return_next_dividend        750 non-null    float64\n",
      "dtypes: float64(5), int64(3), object(8)\n",
      "memory usage: 93.9+ KB\n",
      "\n",
      "Checking number of missing values:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "quarter                                0\n",
       "stock                                  0\n",
       "date                                   0\n",
       "open                                   0\n",
       "high                                   0\n",
       "low                                    0\n",
       "close                                  0\n",
       "volume                                 0\n",
       "percent_change_price                   0\n",
       "percent_change_volume_over_last_wk    30\n",
       "previous_weeks_volume                 30\n",
       "next_weeks_open                        0\n",
       "next_weeks_close                       0\n",
       "percent_change_next_weeks_price        0\n",
       "days_to_next_dividend                  0\n",
       "percent_return_next_dividend           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Check the data types of each column and if there are missing values (0.5 marks)\n",
    "print(\"Checking data types:\\n\")\n",
    "stock_data.info()\n",
    "print(\"\\nChecking number of missing values:\")\n",
    "stock_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17914e32",
   "metadata": {},
   "source": [
    "You should notice in this dataset that there are multiple columns that look numerical, but include a `$` that turns the value into a string (type object). You can use the code below to convert these columns into numerical ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d222ca1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quarter</th>\n",
       "      <th>stock</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>percent_change_price</th>\n",
       "      <th>percent_change_volume_over_last_wk</th>\n",
       "      <th>previous_weeks_volume</th>\n",
       "      <th>next_weeks_open</th>\n",
       "      <th>next_weeks_close</th>\n",
       "      <th>percent_change_next_weeks_price</th>\n",
       "      <th>days_to_next_dividend</th>\n",
       "      <th>percent_return_next_dividend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/7/2011</td>\n",
       "      <td>15.82</td>\n",
       "      <td>16.72</td>\n",
       "      <td>15.78</td>\n",
       "      <td>16.42</td>\n",
       "      <td>239655616</td>\n",
       "      <td>3.79267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.71</td>\n",
       "      <td>15.97</td>\n",
       "      <td>-4.428490</td>\n",
       "      <td>26</td>\n",
       "      <td>0.182704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/14/2011</td>\n",
       "      <td>16.71</td>\n",
       "      <td>16.71</td>\n",
       "      <td>15.64</td>\n",
       "      <td>15.97</td>\n",
       "      <td>242963398</td>\n",
       "      <td>-4.42849</td>\n",
       "      <td>1.380223</td>\n",
       "      <td>239655616.0</td>\n",
       "      <td>16.19</td>\n",
       "      <td>15.79</td>\n",
       "      <td>-2.470660</td>\n",
       "      <td>19</td>\n",
       "      <td>0.187852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/21/2011</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.38</td>\n",
       "      <td>15.60</td>\n",
       "      <td>15.79</td>\n",
       "      <td>138428495</td>\n",
       "      <td>-2.47066</td>\n",
       "      <td>-43.024959</td>\n",
       "      <td>242963398.0</td>\n",
       "      <td>15.87</td>\n",
       "      <td>16.13</td>\n",
       "      <td>1.638310</td>\n",
       "      <td>12</td>\n",
       "      <td>0.189994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/28/2011</td>\n",
       "      <td>15.87</td>\n",
       "      <td>16.63</td>\n",
       "      <td>15.82</td>\n",
       "      <td>16.13</td>\n",
       "      <td>151379173</td>\n",
       "      <td>1.63831</td>\n",
       "      <td>9.355500</td>\n",
       "      <td>138428495.0</td>\n",
       "      <td>16.18</td>\n",
       "      <td>17.14</td>\n",
       "      <td>5.933250</td>\n",
       "      <td>5</td>\n",
       "      <td>0.185989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>2/4/2011</td>\n",
       "      <td>16.18</td>\n",
       "      <td>17.39</td>\n",
       "      <td>16.18</td>\n",
       "      <td>17.14</td>\n",
       "      <td>154387761</td>\n",
       "      <td>5.93325</td>\n",
       "      <td>1.987452</td>\n",
       "      <td>151379173.0</td>\n",
       "      <td>17.33</td>\n",
       "      <td>17.37</td>\n",
       "      <td>0.230814</td>\n",
       "      <td>97</td>\n",
       "      <td>0.175029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quarter stock       date   open   high    low  close     volume  \\\n",
       "0        1    AA   1/7/2011  15.82  16.72  15.78  16.42  239655616   \n",
       "1        1    AA  1/14/2011  16.71  16.71  15.64  15.97  242963398   \n",
       "2        1    AA  1/21/2011  16.19  16.38  15.60  15.79  138428495   \n",
       "3        1    AA  1/28/2011  15.87  16.63  15.82  16.13  151379173   \n",
       "4        1    AA   2/4/2011  16.18  17.39  16.18  17.14  154387761   \n",
       "\n",
       "   percent_change_price  percent_change_volume_over_last_wk  \\\n",
       "0               3.79267                                 NaN   \n",
       "1              -4.42849                            1.380223   \n",
       "2              -2.47066                          -43.024959   \n",
       "3               1.63831                            9.355500   \n",
       "4               5.93325                            1.987452   \n",
       "\n",
       "   previous_weeks_volume  next_weeks_open  next_weeks_close  \\\n",
       "0                    NaN            16.71             15.97   \n",
       "1            239655616.0            16.19             15.79   \n",
       "2            242963398.0            15.87             16.13   \n",
       "3            138428495.0            16.18             17.14   \n",
       "4            151379173.0            17.33             17.37   \n",
       "\n",
       "   percent_change_next_weeks_price  days_to_next_dividend  \\\n",
       "0                        -4.428490                     26   \n",
       "1                        -2.470660                     19   \n",
       "2                         1.638310                     12   \n",
       "3                         5.933250                      5   \n",
       "4                         0.230814                     97   \n",
       "\n",
       "   percent_return_next_dividend  \n",
       "0                      0.182704  \n",
       "1                      0.187852  \n",
       "2                      0.189994  \n",
       "3                      0.185989  \n",
       "4                      0.175029  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Fill-in which columns need the $ to be removed (1 mark)\n",
    "columns = ['open', 'high', 'close', 'low', 'next_weeks_open', 'next_weeks_close']\n",
    "\n",
    "# Code to remove $ - DO NOT CHANGE\n",
    "stock_data[columns] = stock_data[columns].replace('[\\$]', '', regex=True).astype(float)\n",
    "\n",
    "# TO DO: Inspect first few rows to make sure it worked (0.5 marks)\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "32ad28a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750 entries, 0 to 749\n",
      "Data columns (total 16 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   quarter                             750 non-null    int64  \n",
      " 1   stock                               750 non-null    object \n",
      " 2   date                                750 non-null    object \n",
      " 3   open                                750 non-null    float64\n",
      " 4   high                                750 non-null    float64\n",
      " 5   low                                 750 non-null    float64\n",
      " 6   close                               750 non-null    float64\n",
      " 7   volume                              750 non-null    int64  \n",
      " 8   percent_change_price                750 non-null    float64\n",
      " 9   percent_change_volume_over_last_wk  720 non-null    float64\n",
      " 10  previous_weeks_volume               720 non-null    float64\n",
      " 11  next_weeks_open                     750 non-null    float64\n",
      " 12  next_weeks_close                    750 non-null    float64\n",
      " 13  percent_change_next_weeks_price     750 non-null    float64\n",
      " 14  days_to_next_dividend               750 non-null    int64  \n",
      " 15  percent_return_next_dividend        750 non-null    float64\n",
      "dtypes: float64(11), int64(3), object(2)\n",
      "memory usage: 93.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check data type of each column to make sure that the type of the columns selected has changed (0.5 marks)\n",
    "stock_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac4013",
   "metadata": {},
   "source": [
    "The first thing we need to do is deal with missing values. Looking at the dataset, there are two columns with 30 missing values. For this case, we will drop these rows instead of filling them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9fcd2350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quarter</th>\n",
       "      <th>stock</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>percent_change_price</th>\n",
       "      <th>percent_change_volume_over_last_wk</th>\n",
       "      <th>previous_weeks_volume</th>\n",
       "      <th>next_weeks_open</th>\n",
       "      <th>next_weeks_close</th>\n",
       "      <th>percent_change_next_weeks_price</th>\n",
       "      <th>days_to_next_dividend</th>\n",
       "      <th>percent_return_next_dividend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/14/2011</td>\n",
       "      <td>16.71</td>\n",
       "      <td>16.71</td>\n",
       "      <td>15.64</td>\n",
       "      <td>15.97</td>\n",
       "      <td>242963398</td>\n",
       "      <td>-4.428490</td>\n",
       "      <td>1.380223</td>\n",
       "      <td>239655616.0</td>\n",
       "      <td>16.19</td>\n",
       "      <td>15.79</td>\n",
       "      <td>-2.470660</td>\n",
       "      <td>19</td>\n",
       "      <td>0.187852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/21/2011</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.38</td>\n",
       "      <td>15.60</td>\n",
       "      <td>15.79</td>\n",
       "      <td>138428495</td>\n",
       "      <td>-2.470660</td>\n",
       "      <td>-43.024959</td>\n",
       "      <td>242963398.0</td>\n",
       "      <td>15.87</td>\n",
       "      <td>16.13</td>\n",
       "      <td>1.638310</td>\n",
       "      <td>12</td>\n",
       "      <td>0.189994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>1/28/2011</td>\n",
       "      <td>15.87</td>\n",
       "      <td>16.63</td>\n",
       "      <td>15.82</td>\n",
       "      <td>16.13</td>\n",
       "      <td>151379173</td>\n",
       "      <td>1.638310</td>\n",
       "      <td>9.355500</td>\n",
       "      <td>138428495.0</td>\n",
       "      <td>16.18</td>\n",
       "      <td>17.14</td>\n",
       "      <td>5.933250</td>\n",
       "      <td>5</td>\n",
       "      <td>0.185989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>2/4/2011</td>\n",
       "      <td>16.18</td>\n",
       "      <td>17.39</td>\n",
       "      <td>16.18</td>\n",
       "      <td>17.14</td>\n",
       "      <td>154387761</td>\n",
       "      <td>5.933250</td>\n",
       "      <td>1.987452</td>\n",
       "      <td>151379173.0</td>\n",
       "      <td>17.33</td>\n",
       "      <td>17.37</td>\n",
       "      <td>0.230814</td>\n",
       "      <td>97</td>\n",
       "      <td>0.175029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>AA</td>\n",
       "      <td>2/11/2011</td>\n",
       "      <td>17.33</td>\n",
       "      <td>17.48</td>\n",
       "      <td>16.97</td>\n",
       "      <td>17.37</td>\n",
       "      <td>114691279</td>\n",
       "      <td>0.230814</td>\n",
       "      <td>-25.712195</td>\n",
       "      <td>154387761.0</td>\n",
       "      <td>17.39</td>\n",
       "      <td>17.28</td>\n",
       "      <td>-0.632547</td>\n",
       "      <td>90</td>\n",
       "      <td>0.172712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>2</td>\n",
       "      <td>XOM</td>\n",
       "      <td>5/27/2011</td>\n",
       "      <td>80.22</td>\n",
       "      <td>82.63</td>\n",
       "      <td>80.07</td>\n",
       "      <td>82.63</td>\n",
       "      <td>68230855</td>\n",
       "      <td>3.004240</td>\n",
       "      <td>-21.355713</td>\n",
       "      <td>86758820.0</td>\n",
       "      <td>83.28</td>\n",
       "      <td>81.18</td>\n",
       "      <td>-2.521610</td>\n",
       "      <td>75</td>\n",
       "      <td>0.568801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>2</td>\n",
       "      <td>XOM</td>\n",
       "      <td>6/3/2011</td>\n",
       "      <td>83.28</td>\n",
       "      <td>83.75</td>\n",
       "      <td>80.18</td>\n",
       "      <td>81.18</td>\n",
       "      <td>78616295</td>\n",
       "      <td>-2.521610</td>\n",
       "      <td>15.221032</td>\n",
       "      <td>68230855.0</td>\n",
       "      <td>80.93</td>\n",
       "      <td>79.78</td>\n",
       "      <td>-1.420980</td>\n",
       "      <td>68</td>\n",
       "      <td>0.578960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>2</td>\n",
       "      <td>XOM</td>\n",
       "      <td>6/10/2011</td>\n",
       "      <td>80.93</td>\n",
       "      <td>81.87</td>\n",
       "      <td>79.72</td>\n",
       "      <td>79.78</td>\n",
       "      <td>92380844</td>\n",
       "      <td>-1.420980</td>\n",
       "      <td>17.508519</td>\n",
       "      <td>78616295.0</td>\n",
       "      <td>80.00</td>\n",
       "      <td>79.02</td>\n",
       "      <td>-1.225000</td>\n",
       "      <td>61</td>\n",
       "      <td>0.589120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>2</td>\n",
       "      <td>XOM</td>\n",
       "      <td>6/17/2011</td>\n",
       "      <td>80.00</td>\n",
       "      <td>80.82</td>\n",
       "      <td>78.33</td>\n",
       "      <td>79.02</td>\n",
       "      <td>100521400</td>\n",
       "      <td>-1.225000</td>\n",
       "      <td>8.811952</td>\n",
       "      <td>92380844.0</td>\n",
       "      <td>78.65</td>\n",
       "      <td>76.78</td>\n",
       "      <td>-2.377620</td>\n",
       "      <td>54</td>\n",
       "      <td>0.594786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>2</td>\n",
       "      <td>XOM</td>\n",
       "      <td>6/24/2011</td>\n",
       "      <td>78.65</td>\n",
       "      <td>81.12</td>\n",
       "      <td>76.78</td>\n",
       "      <td>76.78</td>\n",
       "      <td>118679791</td>\n",
       "      <td>-2.377620</td>\n",
       "      <td>18.064204</td>\n",
       "      <td>100521400.0</td>\n",
       "      <td>76.88</td>\n",
       "      <td>82.01</td>\n",
       "      <td>6.672740</td>\n",
       "      <td>47</td>\n",
       "      <td>0.612139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     quarter stock       date   open   high    low  close     volume  \\\n",
       "1          1    AA  1/14/2011  16.71  16.71  15.64  15.97  242963398   \n",
       "2          1    AA  1/21/2011  16.19  16.38  15.60  15.79  138428495   \n",
       "3          1    AA  1/28/2011  15.87  16.63  15.82  16.13  151379173   \n",
       "4          1    AA   2/4/2011  16.18  17.39  16.18  17.14  154387761   \n",
       "5          1    AA  2/11/2011  17.33  17.48  16.97  17.37  114691279   \n",
       "..       ...   ...        ...    ...    ...    ...    ...        ...   \n",
       "745        2   XOM  5/27/2011  80.22  82.63  80.07  82.63   68230855   \n",
       "746        2   XOM   6/3/2011  83.28  83.75  80.18  81.18   78616295   \n",
       "747        2   XOM  6/10/2011  80.93  81.87  79.72  79.78   92380844   \n",
       "748        2   XOM  6/17/2011  80.00  80.82  78.33  79.02  100521400   \n",
       "749        2   XOM  6/24/2011  78.65  81.12  76.78  76.78  118679791   \n",
       "\n",
       "     percent_change_price  percent_change_volume_over_last_wk  \\\n",
       "1               -4.428490                            1.380223   \n",
       "2               -2.470660                          -43.024959   \n",
       "3                1.638310                            9.355500   \n",
       "4                5.933250                            1.987452   \n",
       "5                0.230814                          -25.712195   \n",
       "..                    ...                                 ...   \n",
       "745              3.004240                          -21.355713   \n",
       "746             -2.521610                           15.221032   \n",
       "747             -1.420980                           17.508519   \n",
       "748             -1.225000                            8.811952   \n",
       "749             -2.377620                           18.064204   \n",
       "\n",
       "     previous_weeks_volume  next_weeks_open  next_weeks_close  \\\n",
       "1              239655616.0            16.19             15.79   \n",
       "2              242963398.0            15.87             16.13   \n",
       "3              138428495.0            16.18             17.14   \n",
       "4              151379173.0            17.33             17.37   \n",
       "5              154387761.0            17.39             17.28   \n",
       "..                     ...              ...               ...   \n",
       "745             86758820.0            83.28             81.18   \n",
       "746             68230855.0            80.93             79.78   \n",
       "747             78616295.0            80.00             79.02   \n",
       "748             92380844.0            78.65             76.78   \n",
       "749            100521400.0            76.88             82.01   \n",
       "\n",
       "     percent_change_next_weeks_price  days_to_next_dividend  \\\n",
       "1                          -2.470660                     19   \n",
       "2                           1.638310                     12   \n",
       "3                           5.933250                      5   \n",
       "4                           0.230814                     97   \n",
       "5                          -0.632547                     90   \n",
       "..                               ...                    ...   \n",
       "745                        -2.521610                     75   \n",
       "746                        -1.420980                     68   \n",
       "747                        -1.225000                     61   \n",
       "748                        -2.377620                     54   \n",
       "749                         6.672740                     47   \n",
       "\n",
       "     percent_return_next_dividend  \n",
       "1                        0.187852  \n",
       "2                        0.189994  \n",
       "3                        0.185989  \n",
       "4                        0.175029  \n",
       "5                        0.172712  \n",
       "..                            ...  \n",
       "745                      0.568801  \n",
       "746                      0.578960  \n",
       "747                      0.589120  \n",
       "748                      0.594786  \n",
       "749                      0.612139  \n",
       "\n",
       "[720 rows x 16 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Drop rows with missing data (0.5 marks)\n",
    "stock_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b10d14a",
   "metadata": {},
   "source": [
    "### 1.2: Pre-processing (4 marks)\n",
    "\n",
    "In this dataset, we have columns with:\n",
    "- Categorical values\n",
    "- Numerical values\n",
    "\n",
    "We need to create a column transformer that will use the proper preprocessing methods on each type of column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "89e30a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create Column Transformer using an encoder and StandardScaler (1 mark)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "categorical_data = ['stock', 'date']\n",
    "numerical_data = ['quarter', 'open', 'high', 'low', 'close', 'volume', 'percent_change_price', 'percent_change_volume_over_last_wk', 'previous_weeks_volume', 'next_weeks_open', 'next_weeks_close', 'percent_change_next_weeks_price', 'days_to_next_dividend', 'percent_return_next_dividend']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('categorical', OneHotEncoder(), categorical_data),\n",
    "        ('numerical', StandardScaler(), numerical_data),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "38f08685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Initialize your pipeline with your column transformer and the Ridge Regression model (1 mark)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "pipeline = Pipeline(steps = [('preprocessor', preprocessor),\n",
    "                            ('model', Ridge())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c154f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Separate data into feature matrix and target vector (1 mark)\n",
    "X = stock_data.drop('percent_change_next_weeks_price', axis = 1)\n",
    "y = stock_data['percent_change_next_weeks_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "83d815ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Split data into training and testing sets (use random_state=0 and 10% of the data for testing) (0.5 marks)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e626fe",
   "metadata": {},
   "source": [
    "Create another column transformer that does not implement scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "39312544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create a new column transformer that only performs encoding (0.5 marks)\n",
    "preprocessor_encoding = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('categorical', OneHotEncoder(), categorical_data)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd751b7a",
   "metadata": {},
   "source": [
    "### 1.3: Grid Search (4 marks)\n",
    "\n",
    "For the grid search, we want to compare the performance of the Random Forest model to a Ridge Regression model with the two different column transformers. Think about if we need to use scaling for both models. Select parameter values to test that make sense for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5ee11c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create parameter grid and initialize grid object (3 marks)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'model': [Ridge()],\n",
    "    'model__alpha': [0.1, 1, 10],\n",
    "    'preprocessor': [preprocessor]},\n",
    "    {'model': [Ridge()],\n",
    "    'model__alpha': [0.1, 1, 10],\n",
    "    'preprocessor': [preprocessor_encoding]},\n",
    "    {'model': [RandomForestRegressor()],\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'preprocessor': [preprocessor_encoding]}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           param_grid, \n",
    "                           cv = 5, \n",
    "                           scoring = 'r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d4b4e4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('categorical',\n",
       "                                                                         OneHotEncoder(),\n",
       "                                                                         ['stock',\n",
       "                                                                          'date']),\n",
       "                                                                        ('numerical',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['quarter',\n",
       "                                                                          'open',\n",
       "                                                                          'high',\n",
       "                                                                          'low',\n",
       "                                                                          'close',\n",
       "                                                                          'volume',\n",
       "                                                                          'percent_change_price',\n",
       "                                                                          'percent_change_volume_over_last_wk',\n",
       "                                                                          'previous_weeks_volume',\n",
       "                                                                          'next_weeks_open',\n",
       "                                                                          'next_weeks_close',\n",
       "                                                                          'percent_c...\n",
       "                                                                             'percent_return_next_dividend'])])]},\n",
       "                         {'model': [Ridge(alpha=1)],\n",
       "                          'model__alpha': [0.1, 1, 10],\n",
       "                          'preprocessor': [ColumnTransformer(transformers=[('categorical',\n",
       "                                                                            OneHotEncoder(),\n",
       "                                                                            ['stock',\n",
       "                                                                             'date'])])]},\n",
       "                         {'model': [RandomForestRegressor()],\n",
       "                          'model__n_estimators': [100, 200],\n",
       "                          'preprocessor': [ColumnTransformer(transformers=[('categorical',\n",
       "                                                                            OneHotEncoder(),\n",
       "                                                                            ['stock',\n",
       "                                                                             'date'])])]}],\n",
       "             scoring='r2')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Fit grid object to training data (1 mark)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec58f86",
   "metadata": {},
   "source": [
    "### 1.4: Visualize Results (2 marks)\n",
    "\n",
    "The final step is to print out the results from the grid search. You will need to print out the following items:\n",
    "- Best parameters\n",
    "- Best cross-validation train score \n",
    "- Best cross-validation test score\n",
    "- Test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9acc228e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model': Ridge(alpha=1), 'model__alpha': 1, 'preprocessor': ColumnTransformer(transformers=[('categorical', OneHotEncoder(),\n",
      "                                 ['stock', 'date'])])}\n",
      "Best cross-validation train score: 0.30351059743156616\n",
      "Best cross-validation test score: 0.30351059743156616\n",
      "Test set accuracy: 0.3453887088419628\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Print the results from the grid search (2 marks)\n",
    "# Print the results from the grid search\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation train score:\", grid_search.best_score_)\n",
    "print(\"Best cross-validation test score:\", grid_search.cv_results_['mean_test_score'][grid_search.best_index_])\n",
    "print(\"Test set accuracy:\", grid_search.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7324c9",
   "metadata": {},
   "source": [
    "### Questions (8 marks)\n",
    "\n",
    "1. Which models did you use scaling for? Why?\n",
    "1. Which model and what parameters produced the best results?\n",
    "1. Was this model a good fit? Why or why not?\n",
    "1. Is there anything else we could do to try to improve model performance? Provide two ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35ff2b",
   "metadata": {},
   "source": [
    "1. I used scaling for the Ridge Regression model. This is because it is sensitive to scale of the input features. Scaling made it so that all of the features had eual contribution to model performance.\n",
    "2. The RandomForestRegressor model with n_estimators = 200 produced the best results.\n",
    "3. This model was not a very good fit. This is evident when analyzing the test accuracy and cross-validation train/test scores. The best cross-validation scores were approximately 0.3, and the test accuracy was just under 0.3. This signals that the model is underfitting the data.\n",
    "4. Two ways to improve model performance are feature engineering and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd7f37f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2f4ee",
   "metadata": {},
   "source": [
    "1. I sourced my code from class content, such as labs and lecture examples, as well as through the internet and generative AI.\n",
    "2. I completed the steps in the order they were given.\n",
    "3. I used generative AI when I hit roadblocks or errors. I provided prompts such as \"given the following information, can you analyze and explain the results\" as well as providing it with pieces of completed work and asking it to finish the TODO at points where I was stuck. I did not have to modify the code much as it was often used for debugging or explanations, and when it was used for actual code generation the lines were small enough that adjustment was not required.\n",
    "4. Having a good understanding of the lecture content and going over the lecture/lab examples was helpful in eliminating challenges through the process of completing part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1446bc2",
   "metadata": {},
   "source": [
    "## Part 2: Text Data (32 marks)\n",
    "\n",
    "The purpose of this part of the assignment is to practice working with text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c0a90",
   "metadata": {},
   "source": [
    "### 2.1: Load data (1 mark)\n",
    "For this task, we will be using the hobbies dataset from the yellowbrick library. More information on the dataset can be found here: https://www.scikit-yb.org/en/latest/api/datasets/hobbies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "24ef6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Load the dataset (1 mark)\n",
    "import yellowbrick\n",
    "from yellowbrick.datasets import load_hobbies\n",
    "data = load_hobbies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f72f48e",
   "metadata": {},
   "source": [
    "### 2.2 Pre-processing (3 marks)\n",
    "\n",
    "We will need to transform the data from strings to numeric. First, we will transform the data using `CountVectorizer(min_df=5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b1deaaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create CountVectorizer object (0.5 marks)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "25cbc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Fit vectorizer to data (0.5 marks)\n",
    "fit = vectorizer.fit(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c812613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 3940\n"
     ]
    }
   ],
   "source": [
    "# TO DO: What is the length of the vocabulary? (0.5 marks)\n",
    "vocab_length = len(vectorizer.vocabulary_)\n",
    "print(\"length:\", vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1696f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Transform the data (0.5 marks)\n",
    "X = vectorizer.transform(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "57bedc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (448, 3940)\n"
     ]
    }
   ],
   "source": [
    "# TO DO: What is the shape of the transformed data? (0.5 marks)\n",
    "print(\"Shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4f7e03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Split data into training and testing sets (use random_state=0 and 10% of the data for testing) (0.5 marks)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data.target, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d11f4e",
   "metadata": {},
   "source": [
    "### 2.3: Grid Search (5 marks)\n",
    "\n",
    "For the grid search, we want to compare the performance of Logistic Regression for different values of C. Initialize the parameter grid with parameter values that make sense for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "137d6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create parameter grid and initialize grid object (2 marks)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "logistic = LogisticRegression(max_iter = 1000)\n",
    "grid_search = GridSearchCV(logistic, param_grid, cv=5, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "80113917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=1000),\n",
       "             param_grid={'C': [0.1, 1, 10, 100]}, scoring='accuracy')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Fit grid object to training data (1 mark)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3995afa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1}\n",
      "Best Cross-Validation Accuracy: 0.7916049382716049\n",
      "Test Set Accuracy: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Print the results from the grid search (2 marks)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "print(\"Test Set Accuracy:\", grid_search.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31294d74",
   "metadata": {},
   "source": [
    "### 2.4: Additional Model Comparisons (9 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca2da7",
   "metadata": {},
   "source": [
    "### 2.4.1: Naive Bayes (3 marks)\n",
    "We would like to compare the performance of Logistic Regression with one of the Naive Bayes models. Pick the Naive Bayes model that you think would best suit text data and implement below. Since we are not adjusting hyperparameters, we can use `cross_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ce5b6ab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.960915716400977\n",
      "Validation accuracy: 0.8808641975308642\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Implement Naive Bayes model with cross-validate (2 marks)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "multinomial = MultinomialNB()\n",
    "results = cross_validate(multinomial, X_train, y_train, cv = 5, scoring = 'accuracy', return_train_score = True)\n",
    "\n",
    "# TO DO: Print training and validation accuracies\n",
    "print(\"Training accuracy:\", results['train_score'].mean())\n",
    "print(\"Validation accuracy:\", results['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d3d505f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Calculate and print test accuracy (1 mark)\n",
    "multinomial.fit(X_train, y_train)\n",
    "test_accuracy = multinomial.score(X_test, y_test)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ed1f4",
   "metadata": {},
   "source": [
    "### 2.4.2 Tf-idf (6 marks)\n",
    "\n",
    "To try to improve the results, we can try using Tf-idf to tranform the text data based on the importance of each feature. We will need to use a pipeline and the original data for this section. Use `TfidfVectorizer(min_df=5)` and compare the results for both Logistic Regression and your selected Naive Bayes model. Use the Logistic Regression parameters from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "49ddef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Split the data into training and testing sets (same values as previous section) (1 mark)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_vect, X_test_vect, y_train_vect, y_test_vect = train_test_split(data.data, data.target, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d1b7a2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(min_df=5)),\n",
       "                ('model', MultinomialNB())])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Implement Pipeline with Tf-idf vectorizer and both Logistic Regression and your selected Naive Bayes model (3 marks)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "logistic_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(min_df = 5)),\n",
    "    ('model', LogisticRegression(max_iter = 1000, C=grid_search.best_params_['C']))\n",
    "])\n",
    "\n",
    "multinomial_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(min_df = 5)),\n",
    "    ('model', MultinomialNB())\n",
    "])\n",
    "\n",
    "logistic_pipeline.fit(X_train_vect, y_train_vect)\n",
    "multinomial_pipeline.fit(X_train_vect, y_train_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ea92cc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic accuracy: 0.8666666666666667\n",
      "Multinomial accuracy: 0.8222222222222222\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Print the results from the grid search (2 marks)\n",
    "logistic_accuracy = logistic_pipeline.score(X_test_vect, y_test_vect)\n",
    "multinomial_accuracy = multinomial_pipeline.score(X_test_vect, y_test_vect)\n",
    "print(\"Logistic accuracy:\", logistic_accuracy)\n",
    "print(\"Multinomial accuracy:\", multinomial_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aee78b",
   "metadata": {},
   "source": [
    "### Questions (10 marks)\n",
    "\n",
    "1. Which Naive Bayes model did you pick? Why?\n",
    "1. Which model and what parameters produced the best results?\n",
    "1. Was this model a good fit? Why or why not?\n",
    "1. Is there anything else we could do to try to improve model performance? Provide two ideas (must be different from Part 1).\n",
    "1. Why did we need to implement a pipeline for Tf-idf and not CountVectorizer? What would happen if we didn't use one for Tf-idf?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323788fa",
   "metadata": {},
   "source": [
    "1. I chose the Multinomial Naive Bayes model as it is best for text data, which was used here.\n",
    "2. The best model was MultinomialNB. The validation accuracy was 0.88, and the test and training accuracy were both over 0.95, which is significantly better than the other models tested. \n",
    "3. Based on the numerical results, the model fit the training data well and generalized well in cross-validation. It also performed well on the testing set, all of which makes the model a good fit for the data.\n",
    "4. Two ways to improve the results are to use text-preprocessing to remove unimportant words, or to use multiple models in an ensemble method.\n",
    "5. A pipeline was implemented for Td-idf because it ensures that the Tf-idf transformation is consistent on both the test and training data. This step is not necessary for CountVectorizer as it does not rely on the datasets global statistics. Without a pipeline, the Tf-idf vectorizer may fit the entire dataset, including the test data, which would change the models performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40621d7",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc92f7",
   "metadata": {},
   "source": [
    "1. I sourced my code from lecture content, yellowbrick documentation, and generative AI.\n",
    "2. I completed the steps in the order they were given.\n",
    "3. I used generative AI to debug and answer questions about models, and any code provided was small enough so as to not require modification.\n",
    "4. Reading outside documentation and looking through lecture/lab examples was very helpful in understanding the questions being asked in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc65ad6",
   "metadata": {},
   "source": [
    "## Part 3: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "I appreciated the layout and the breakdown of marks in each code cell. I found grid search very intriguing. I also found the model comparison interesting and felt as though it helped elevate my understanding of the various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9752e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
